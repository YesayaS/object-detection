{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ObjectDetection_POC_KopiOey.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1yHkC_wWUE9Xhmppiw5RKkNuT4D8NsF9W",
      "authorship_tag": "ABX9TyMqCNuwJnQI0oti2rL04jPB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/corvus-rex/object-detection/blob/master/ObjectDetection_POC_KopiOey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwr_20uKZnH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install protobuf, cython and tf-slim\n",
        "!apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install Cython\n",
        "!pip install tf-slim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-WHkgyRb9Fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clone the master repo\n",
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj3fQIdleqDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile proto files\n",
        "# make sure you are in BASE_DIR/research/ (see cell below)\n",
        "%cd /content/drive/My Drive/Laeveteinn/KopiOey/models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6MOXzIdnv-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c654bb96-3d15-45c3-8144-fb262b6309e7"
      },
      "source": [
        "# Add path environment\n",
        "PERSONAL_DIR = \"/content/drive/My Drive/Laeveteinn/KopiOey/\"\n",
        "BASE_DIR = PERSONAL_DIR + \"models/\"\n",
        "%set_env PYTHONPATH=/content/drive/My Drive/Laeveteinn/KopiOey/models/research:/content/drive/My Drive/Laeveteinn/KopiOey/models/research/slim\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/drive/My Drive/Laeveteinn/KopiOey/models\"\n",
        "os.environ['PYTHONPATH'] += \":/content/drive/My Drive/Laeveteinn/KopiOey/pyimagesearch\"\n",
        "import sys\n",
        "sys.path.insert(1, os.path.join(BASE_DIR, \"official\"))\n",
        "sys.path.insert(1, os.path.join(PERSONAL_DIR, \"pyimagesearch\"))\n",
        "print(sys.path)\n",
        "\n",
        "# Run build test\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=/content/drive/My Drive/Laeveteinn/KopiOey/models/research:/content/drive/My Drive/Laeveteinn/KopiOey/models/research/slim\n",
            "['', '/content/drive/My Drive/Laeveteinn/KopiOey/pyimagesearch', '/content/drive/My Drive/Laeveteinn/KopiOey/models/official', '/content/drive/My Drive/Laeveteinn/KopiOey/pyimagesearch', '/content/drive/My Drive/Laeveteinn/KopiOey/models/official', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "2020-08-28 07:08:48.181511: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5NWU1r3y0N_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c5ac4cf-17af-4252-dfc2-432c8c93d8a2"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import PIL\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWMYwk46mnKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize variables\n",
        "MODELS = [\"centernet_resnet101_v1_fpn_512x512_coco17_tpu-8\",\n",
        "              \"centernet_hg104_512x512_coco17_tpu-8\"]\n",
        "MODEL_NAME = MODELS[1]\n",
        "BASE_DOWNLOAD = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/\"\n",
        "DOWNLOAD_URL = BASE_DOWNLOAD + MODEL_NAME + \".tar.gz\"\n",
        "MODEL_FILE = MODEL_NAME + \".tar.gz\""
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3lkHgBx5l-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and extract tar file\n",
        "import six.moves.urllib as urllib\n",
        "import tarfile\n",
        "\n",
        "opener = urllib.request.URLopener()\n",
        "opener.retrieve(DOWNLOAD_URL, MODEL_FILE)\n",
        "tar_file = tarfile.open(MODEL_FILE)\n",
        "tar_file.extractall(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJf_GA5xCdex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42125e59-10fd-4308-a54d-ac6101e90723"
      },
      "source": [
        "# Config and checkpoint dir\n",
        "config_dir = BASE_DIR + \"/research/object_detection/\" + MODEL_NAME\n",
        "pipeline_config = os.path.join(config_dir, \"pipeline.config\")\n",
        "model_dir = BASE_DIR + \"research/object_detection/\" + MODEL_NAME + \"/checkpoint\"\n",
        "\n",
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=False)\n",
        "\n",
        "# Restore checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(\n",
        "      model=detection_model)\n",
        "ckpt.restore(os.path.join(model_dir, 'ckpt-0')).expect_partial()\n",
        "\n",
        "print(pipeline_config)\n",
        "def get_model_detection_function(model):\n",
        "  \"\"\"Get a tf.function for detection.\"\"\"\n",
        "\n",
        "  @tf.function\n",
        "  def detect_fn(image):\n",
        "    \"\"\"Detect objects in image.\"\"\"\n",
        "\n",
        "    image, shapes = model.preprocess(image)\n",
        "    prediction_dict = model.predict(image, shapes)\n",
        "    detections = model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
        "\n",
        "  return detect_fn\n",
        "\n",
        "detect_fn = get_model_detection_function(detection_model)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Laeveteinn/KopiOey/models//research/object_detection/centernet_hg104_512x512_coco17_tpu-8/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GQIHALBG3dV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "406acc18-b5e8-4f23-b333-0fc1513bcdf3"
      },
      "source": [
        "%cd ..\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Laeveteinn/KopiOey/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3RNw1joSxGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = PIL.Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwmmTZwvCMvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only detect people?\n",
        "DETECT_HUMAN = True"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y3BwHsW5R_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label map\n",
        "if DETECT_HUMAN:\n",
        "  label_map_path = BASE_DIR + \"research/object_detection/data/mscoco_peopleonly_label_map.pbtxt\"\n",
        "else:\n",
        "  label_map_path = BASE_DIR + \"research/object_detection/data/mscoco_label_map.pbtxt\"\n",
        "label_map = label_map_util.load_labelmap(label_map_path)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map,\n",
        "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
        "    use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9VwaRAPVB2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing model with still frame image\n",
        "\n",
        "from IPython.display import clear_output, Image\n",
        "import base64\n",
        "import cv2\n",
        "\n",
        "# Image Preparation\n",
        "image_dir = PERSONAL_DIR + \"evaluation_data/\"\n",
        "image_path = os.path.join(image_dir, 'stillframe0.jpg')\n",
        "image_np = load_image_into_numpy_array(image_path)  # load image as np array\n",
        "\n",
        "# Detection\n",
        "input_tensor = tf.convert_to_tensor(\n",
        "    np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
        "\n",
        "label_id_offset = 1  # COCO dataset Class starts at index 1\n",
        "image_np_with_detections = image_np.copy()  # for matplotlib display\n",
        "\n",
        "if DETECT_HUMAN:\n",
        "  selected_class = 1  # only selects one class from COCO dataset, 'Person'\n",
        "\n",
        "# If Detect Human only is turned on, only display boxes, classes and scores\n",
        "# that detects human in the image\n",
        "if DETECT_HUMAN:\n",
        "  # Indices where class == human\n",
        "  indices = np.squeeze(np.argwhere((detections['detection_classes'][0] + label_id_offset) == selected_class))\n",
        "  boxes = detections['detection_boxes'][0].numpy()\n",
        "  boxes = boxes[[indices]]  # select corresponding array indices of the 4-point box\n",
        "  classes = (detections['detection_classes'][0] + label_id_offset).numpy()\n",
        "  classes = classes[[indices]]  # select corresponding array of the detected class\n",
        "  scores = detections['detection_scores'][0].numpy()\n",
        "  scores = scores[[indices]]  # select corresponding scores of the detected class\n",
        "  print(boxes)\n",
        "  print(classes)\n",
        "  print(boxes.shape[0])\n",
        "else:\n",
        "  boxes = detections['detection_boxes'][0].numpy()\n",
        "  classes = (detections['detection_classes'][0].numpy() + label_id_offset).astype(int)\n",
        "  scores = detections['detection_scores'][0].numpy()\n",
        "\n",
        "# Draw boxes and labels on the frame\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections,\n",
        "      boxes,\n",
        "      classes,\n",
        "      scores,\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=200,\n",
        "      min_score_thresh=.25,  # minimum score for object to be considered as detected\n",
        "      agnostic_mode=False)\n",
        "\n",
        "# Matplotlib\n",
        "plt.figure(figsize=(12,16))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67p4rQ-u8BBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following has been modified to fit TF2's vis util box normalized bounding box\n",
        "# import the necessary packages\n",
        "from scipy.spatial import distance as dist\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "class CentroidTracker():\n",
        "\tdef __init__(self, frame_shape, maxDisappeared=50):\n",
        "\t\t# initialize the next unique object ID along with two ordered\n",
        "\t\t# dictionaries used to keep track of mapping a given object\n",
        "\t\t# ID to its centroid and number of consecutive frames it has\n",
        "\t\t# been marked as \"disappeared\", respectively\n",
        "\t\tself.nextObjectID = 0\n",
        "\t\tself.objects = OrderedDict()\n",
        "\t\tself.disappeared = OrderedDict()\n",
        "\t\t# Height, width and color channel of the frame\n",
        "\t\tself.height, self.width, self.channel = frame_shape\n",
        "\n",
        "\t\t# store the number of maximum consecutive frames a given\n",
        "\t\t# object is allowed to be marked as \"disappeared\" until we\n",
        "\t\t# need to deregister the object from tracking\n",
        "\t\tself.maxDisappeared = maxDisappeared\n",
        "\n",
        "\tdef register(self, centroid):\n",
        "\t\t# when registering an object we use the next available object\n",
        "\t\t# ID to store the centroid\n",
        "\t\tself.objects[self.nextObjectID] = centroid\n",
        "\t\tself.disappeared[self.nextObjectID] = 0\n",
        "\t\tself.nextObjectID += 1\n",
        "\n",
        "\tdef deregister(self, objectID):\n",
        "\t\t# to deregister an object ID we delete the object ID from\n",
        "\t\t# both of our respective dictionaries\n",
        "\t\tdel self.objects[objectID]\n",
        "\t\tdel self.disappeared[objectID]\n",
        "\n",
        "\tdef update(self, rects):\n",
        "\t\t# check to see if the list of input bounding box rectangles\n",
        "\t\t# is empty\n",
        "\t\tif len(rects) == 0:\n",
        "\t\t\t# loop over any existing tracked objects and mark them\n",
        "\t\t\t# as disappeared\n",
        "\t\t\tfor objectID in list(self.disappeared.keys()):\n",
        "\t\t\t\tself.disappeared[objectID] += 1\n",
        "\n",
        "\t\t\t\t# if we have reached a maximum number of consecutive\n",
        "\t\t\t\t# frames where a given object has been marked as\n",
        "\t\t\t\t# missing, deregister it\n",
        "\t\t\t\tif self.disappeared[objectID] > self.maxDisappeared:\n",
        "\t\t\t\t\tself.deregister(objectID)\n",
        "\n",
        "\t\t\t# return early as there are no centroids or tracking info\n",
        "\t\t\t# to update\n",
        "\t\t\treturn self.objects\n",
        "\n",
        "\t\t# initialize an array of input centroids for the current frame\n",
        "\t\tinputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
        "\n",
        "\t\t# loop over the bounding box rectangles\n",
        "\t\tfor (i, (startY, startX, endY, endX)) in enumerate(rects):\n",
        "\t\t\t# denormalize the bounding box coordinate\n",
        "\t\t\td_minX = startX * self.width\n",
        "\t\t\td_maxX = endX * self.width\n",
        "\t\t\td_minY = startY * self.height\n",
        "\t\t\td_maxY = endY * self.height\n",
        "\t\t\t# use the bounding box coordinates to derive the centroid\n",
        "\t\t\tcX = int((d_minX + d_maxX) / 2.0)\n",
        "\t\t\tcY = int((d_minY + d_maxY) / 2.0)\n",
        "\t\t\tinputCentroids[i] = (cX, cY)\n",
        "\n",
        "\t\t# if we are currently not tracking any objects take the input\n",
        "\t\t# centroids and register each of them\n",
        "\t\tif len(self.objects) == 0:\n",
        "\t\t\tfor i in range(0, len(inputCentroids)):\n",
        "\t\t\t\tself.register(inputCentroids[i])\n",
        "\n",
        "\t\t# otherwise, are are currently tracking objects so we need to\n",
        "\t\t# try to match the input centroids to existing object\n",
        "\t\t# centroids\n",
        "\t\telse:\n",
        "\t\t\t# grab the set of object IDs and corresponding centroids\n",
        "\t\t\tobjectIDs = list(self.objects.keys())\n",
        "\t\t\tobjectCentroids = list(self.objects.values())\n",
        "\n",
        "\t\t\t# compute the distance between each pair of object\n",
        "\t\t\t# centroids and input centroids, respectively -- our\n",
        "\t\t\t# goal will be to match an input centroid to an existing\n",
        "\t\t\t# object centroid\n",
        "\t\t\tD = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
        "\n",
        "\t\t\t# in order to perform this matching we must (1) find the\n",
        "\t\t\t# smallest value in each row and then (2) sort the row\n",
        "\t\t\t# indexes based on their minimum values so that the row\n",
        "\t\t\t# with the smallest value as at the *front* of the index\n",
        "\t\t\t# list\n",
        "\t\t\trows = D.min(axis=1).argsort()\n",
        "\n",
        "\t\t\t# next, we perform a similar process on the columns by\n",
        "\t\t\t# finding the smallest value in each column and then\n",
        "\t\t\t# sorting using the previously computed row index list\n",
        "\t\t\tcols = D.argmin(axis=1)[rows]\n",
        "\n",
        "\t\t\t# in order to determine if we need to update, register,\n",
        "\t\t\t# or deregister an object we need to keep track of which\n",
        "\t\t\t# of the rows and column indexes we have already examined\n",
        "\t\t\tusedRows = set()\n",
        "\t\t\tusedCols = set()\n",
        "\n",
        "\t\t\t# loop over the combination of the (row, column) index\n",
        "\t\t\t# tuples\n",
        "\t\t\tfor (row, col) in zip(rows, cols):\n",
        "\t\t\t\t# if we have already examined either the row or\n",
        "\t\t\t\t# column value before, ignore it\n",
        "\t\t\t\t# val\n",
        "\t\t\t\tif row in usedRows or col in usedCols:\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t# otherwise, grab the object ID for the current row,\n",
        "\t\t\t\t# set its new centroid, and reset the disappeared\n",
        "\t\t\t\t# counter\n",
        "\t\t\t\tobjectID = objectIDs[row]\n",
        "\t\t\t\tself.objects[objectID] = inputCentroids[col]\n",
        "\t\t\t\tself.disappeared[objectID] = 0\n",
        "\n",
        "\t\t\t\t# indicate that we have examined each of the row and\n",
        "\t\t\t\t# column indexes, respectively\n",
        "\t\t\t\tusedRows.add(row)\n",
        "\t\t\t\tusedCols.add(col)\n",
        "\n",
        "\t\t\t# compute both the row and column index we have NOT yet\n",
        "\t\t\t# examined\n",
        "\t\t\tunusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
        "\t\t\tunusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
        "\n",
        "\t\t\t# in the event that the number of object centroids is\n",
        "\t\t\t# equal or greater than the number of input centroids\n",
        "\t\t\t# we need to check and see if some of these objects have\n",
        "\t\t\t# potentially disappeared\n",
        "\t\t\tif D.shape[0] >= D.shape[1]:\n",
        "\t\t\t\t# loop over the unused row indexes\n",
        "\t\t\t\tfor row in unusedRows:\n",
        "\t\t\t\t\t# grab the object ID for the corresponding row\n",
        "\t\t\t\t\t# index and increment the disappeared counter\n",
        "\t\t\t\t\tobjectID = objectIDs[row]\n",
        "\t\t\t\t\tself.disappeared[objectID] += 1\n",
        "\n",
        "\t\t\t\t\t# check to see if the number of consecutive\n",
        "\t\t\t\t\t# frames the object has been marked \"disappeared\"\n",
        "\t\t\t\t\t# for warrants deregistering the object\n",
        "\t\t\t\t\tif self.disappeared[objectID] > self.maxDisappeared:\n",
        "\t\t\t\t\t\tself.deregister(objectID)\n",
        "\n",
        "\t\t\t# otherwise, if the number of input centroids is greater\n",
        "\t\t\t# than the number of existing object centroids we need to\n",
        "\t\t\t# register each new input centroid as a trackable object\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor col in unusedCols:\n",
        "\t\t\t\t\tself.register(inputCentroids[col])\n",
        "\n",
        "\t\t# return the set of trackable objects\n",
        "\t\treturn self.objects"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCBTHUVUmqry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "09759e1a-bfb5-4c39-ea8c-20bff118399d"
      },
      "source": [
        "import cv2\n",
        "import time\n",
        "\n",
        "eval_dir = PERSONAL_DIR + \"evaluation_data/\"\n",
        "video_path = eval_dir + \"sample1.mp4\"\n",
        "out_dir = eval_dir + \"out_hourglass1.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "res=(928,576)  #resolution\n",
        "fourcc = cv2.VideoWriter_fourcc('M','J','P','G') #codec\n",
        "out = cv2.VideoWriter(out_dir, fourcc, 20.0, res)\n",
        "label_id_offset = 1  # COCO dataset Class starts at index 1\n",
        "frame_shape = list(res)\n",
        "frame_shape.append(3)\n",
        "Tracker = CentroidTracker(frame_shape, maxDisappeared=50)\n",
        "\n",
        "if DETECT_HUMAN:\n",
        "  selected_class = 1  # only selects one class from COCO dataset, 'Person'\n",
        "\n",
        "duration = 50\n",
        "start_time = time.time()\n",
        "while (int(time.time() - start_time) <= duration):\n",
        "  ret, frame_np = cap.read()\n",
        "\n",
        "  if not ret:\n",
        "    break\n",
        "\n",
        "  # Detection\n",
        "  input_tensor = tf.convert_to_tensor(np.expand_dims(frame_np, 0), dtype=tf.float32)\n",
        "  detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
        "\n",
        "  frame_np_with_detections = frame_np.copy()  # for matplotlib display\n",
        "\n",
        "  # If Detect Human only is turned on, only display boxes, classes and scores\n",
        "  # that detects human in the image\n",
        "  if DETECT_HUMAN:\n",
        "    # Indices where class == human\n",
        "    indices = np.squeeze(np.argwhere((detections['detection_classes'][0] + label_id_offset) == selected_class))\n",
        "    boxes = detections['detection_boxes'][0].numpy()\n",
        "    boxes = boxes[[indices]]  # select corresponding array indices of the 4-point box\n",
        "    classes = (detections['detection_classes'][0] + label_id_offset).numpy()\n",
        "    classes = classes[[indices]]  # select corresponding array of the detected class\n",
        "    scores = detections['detection_scores'][0].numpy()\n",
        "    scores = scores[[indices]]  # select \n",
        "  else:\n",
        "    boxes = detections['detection_boxes'][0].numpy()\n",
        "    classes = (detections['detection_classes'][0].numpy() + label_id_offset).astype(int)\n",
        "    scores = detections['detection_scores'][0].numpy()\n",
        "\n",
        "  People = Tracker.update(boxes)\n",
        "  red = (255,0,0)\n",
        "  for key, value in People.items():\n",
        "    value = tuple(value)\n",
        "    cv2.circle(frame_np_with_detections, value, 1, red, -20)\n",
        "\n",
        "  # Draw boxes and labels on the frame\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        frame_np_with_detections,\n",
        "        boxes,\n",
        "        classes,\n",
        "        scores,\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=200,\n",
        "        min_score_thresh=.30,\n",
        "        agnostic_mode=False)\n",
        "  \n",
        "  out.write(frame_np_with_detections)\n",
        "\n",
        "cap.release()\n",
        "out.release()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkqHDiSgVCjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e17b85e7-f1c8-4782-ca60-f3978fa9cf36"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "coords0 = [(35.0456, -85.2672),\n",
        "          (35.1174, -89.9711),\n",
        "          (35.9728, -83.9422),\n",
        "          (36.1667, -86.7833)]\n",
        "coords1 = [(35.2346, -82.0923),\n",
        "          (31.3567, -81.3591),\n",
        "          (33.6858, -86.1278)]\n",
        "D = distance.cdist(coords0, coords1, 'euclidean')\n",
        "print(D)\n",
        "rows = D.min(axis=1).argsort()\n",
        "print(rows)\n",
        "cols = D.argmin(axis=1)[rows]\n",
        "print(cols)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.18052056 5.37412587 1.60925088]\n",
            " [7.87967165 9.39730858 4.10127217]\n",
            " [1.9917503  5.28968665 3.16341846]\n",
            " [4.78270754 7.2496928  2.56603684]]\n",
            "[0 2 3 1]\n",
            "[2 0 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}